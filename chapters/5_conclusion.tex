
\chapter{Conclusion \& Future Work}
\label{conclusion}
In the previous section, a number of changes have been proposed that allow training a single autoencoder that can model a wide range of sounds from different instruments.
While the baseline DDSP model with $z$-encoder can also reconstruct sounds of different instruments, it is not straightforward to synthesize new realistic sounds with it, since it is unclear how to obtain a good sequence of $z$-encodings.
We can solve this problem by using a single timbre vector extracted from a recording rather than time-distributed features. The time-constant $z$ models produce better audios for timbre transfer that sound less noisy, and this can be measured using the cycle-reconstruction loss. \newline
Two other modifications, namely the adapted way of calculating loudness and the use of u-MSS as a loss function, have been shown to further improve the performance of the model and allow the synthesis of near-realistic sounds. \newline
Even better quality can be achieved by fine-tuning the model on the target timbre. A pretrained general model quickly learns to adapt to a specific instrument, requiring only a minute of training data even for completely new instruments. How to perform fine-tuning has implications on how the model can be used for other tasks besides reconstruction or timbre transfer. \newline
In this chapter, a number of potential applications and future directions are discussed.
A huge constraint of the models considered so far is that they only work on clean monophonic audio. To overcome this, \Cref{polyphonic} suggests an idea on how to perform audio source separation jointly with encoding each source into a similar latent representation as it is done in the monophonic case. \newline
Another exciting direction is building models to obtain higher-level abstractions of audio, such as MIDI. As MIDI is a widely used format for music, models that decode MIDI can easily be used with existing tools to create music. Other works like \citet{music-transformer} use transformers to generate music as a sequence of MIDI events. A combination of a language model for MIDI sequences with another model that decodes MIDI to realistic waveforms can therefore be used to generate new music in an interpretable and modifiable way. \newline
Abstractions like MIDI necessarily discard information on details. If neural networks can freely choose their latent encodings, it relies on the loss function to guide the network towards representations that preserve the relevant information. In the case of compressing audio, the relevant information is determined by human perception and approximated using loss functions like the MSS loss. If a model cannot freely choose the structure of the latent representation - for example, if we train it to encode to MIDI or the $f_0, ld, z$ representation - then this representation can limit the information that can be recovered. In order to synthesize singing, more information than MIDI can encode is required: lyrics or details of pronunciation are beyond the scope of MIDI. An idea on how to combine the advantages of human informed high-level abstractions with the flexibility of unconstraint latent representations is discussed in \Cref{sec:again-time-distributed}.
\newline


\section{Polyphonic Audio}
\label{polyphonic}
Monophonic audio rarely occurs in the wild. It would therefore be useful to build an autoencoder that can separate different audio sources from a mixed signal such that each source can be modeled separately in a latent code like $z, f_0, ld$. This problem could be approached in various ways, with the most obvious being to use one encoder that controls $k$ decoders by outputting $k$ latent vectors. The decoders can be pretrained on monophonic audio as their role is to synthesize single monophonic sources of the mixed signal. \newline
Another approach is to feed the input repeatedly to the model and extract monophonic audio sources iteratively. In this case, the encoder would have two inputs: one for the mixed signal and one for the summed previous outputs of the decoder. \newline
One obstacle to training these two approaches in an unsupervised manner is that loudness and pitch cannot be extracted as easily as they can be with monophonic audio. Even the monophonic autoencoder cannot learn to extract $f_0$ in an unsupervised way, which is why CREPE is used. An interesting analysis of reasons for this is done by \citet{turian_im_nodate} and additional experiments regarding this are described in \Cref{oscillating-loss}. \newline
On the other hand, it is easy to train a supervised model that can extract $f_0$ of a monophonic audio sample.
Therefore, one idea to teach the model to deal with polyphonic audio is to use a constructed dataset of mixed monophonic sounds for training, such that $f_0$ and loudness can be learned in a supervised way, while $z$ is learned in an unsupervised way. \newline
If it succeeds, the resulting autoencoder can then separate the individual audio sources by considering each decoder's output individually. The findings of \Cref{transfer} suggest that fine-tuning on each sample can help to improve the reconstruction accuracy. This can also be evaluated in the context of polyphonic audio: here, the $f_0$ encoder would be fixed as we know that the gradients provide a poor learning signal, but the $z$ encoder and the decoders can adapt to the specific sample. \newline

\section{Higher-level Abstractions}
\label{high}
Unconstrained latent representations obtained by autoencoders are difficult to manipulate in a targeted way. The power of the DDSP autoencoder discussed in this thesis arises from the latent code that follows a human-designed structure, such that we can easily modify only the melody or the timbre. \newline
Certain combinations of melody and $z$-vector, however, sound poor. A melody extracted from singing transferred to the timbre of a piano is an example of this phenomenon. The reason lies in the fact that the pitch and loudness curves are also typical of certain instruments: a piano note is always played loudest shortly after the key is hit - this is called the attack of the amplitude envelope - and then decays. The $f_0$ and loudness curves of singing, on the other hand, are much less constrained. \newline
To overcome this limitation, higher-level abstractions can be explored. A widely used format in music production is MIDI. MIDI are control sequences for electrical keyboards that control synthesizers. It assumes a discrete set of notes that can be pressed with velocities of a certain range. Different synthesizers can decode the same MIDI to waveforms with different timbre. The pitch and loudness of the generated audio also depend on the specific synthesizer used because these are lower-level descriptors than MIDI. \newline
Therefore, an interesting direction of research is to build an additional autoencoder that learns to extract a MIDI-like encoding on top of the $f_0, ld, z$ representation. The new decoder would then learn to predict a realistic $f_0, ld$ curve from the MIDI sequence and the $z$ vector. Therefore, the model can learn to adjust the details of the melody to the instrument it tries to model and hopefully produce a more realistic sound for timbre transfer. \newline
Additionally, this can be combined with existing work on modeling MIDI sequences to generate music \citep{performance-rnn-2017} \citep{roberts_hierarchical_2019}. Artists can use it to co-create music and modify either the MIDI level or dimensions of $z$. \newline



\section{Combining Human-Informed Abstractions with Unconstrained Latent Representations}
\label{sec:again-time-distributed}
While it is an advantage to abstract away details sometimes, it can also be a limitation. Models discussed so far all lack the flexibility to generate complex audio like singing, which has important time-varying features besides pitch and loudness. Models like VQ-VAEs with a less constrained latent space have this power but lack controllability. \newline
As another potential line of future work, it can be explored how to combine the advantages of human-informed abstractions with the flexibility of unconstrained latent representations. One idea is to reintroduce a time-varying latent variable in addition to the time-constant timbre vector and use regularization techniques to reward the model for putting as much information as possible into the most abstract latent representation - i.e. to reward using the timbre vector to encode information over using the time-distributed latent variable. In the case of singing, a desirable outcome would be to disentangle the voice, which corresponds to the time-constant part, from the lyrics.
