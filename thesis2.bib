
@article{chen_gradnorm_2018,
	title = {{GradNorm}: {Gradient} {Normalization} for {Adaptive} {Loss} {Balancing} in {Deep} {Multitask} {Networks}},
	shorttitle = {{GradNorm}},
	url = {http://arxiv.org/abs/1711.02257},
	abstract = {Deep multitask networks, in which one neural network produces multiple predictive outputs, can offer better speed and performance than their single-task counterparts but are challenging to train properly. We present a gradient normalization (GradNorm) algorithm that automatically balances training in deep multitask models by dynamically tuning gradient magnitudes. We show that for various network architectures, for both regression and classification tasks, and on both synthetic and real datasets, GradNorm improves accuracy and reduces overfitting across multiple tasks when compared to single-task networks, static baselines, and other adaptive multitask loss balancing techniques. GradNorm also matches or surpasses the performance of exhaustive grid search methods, despite only involving a single asymmetry hyperparameter \${\textbackslash}alpha\$. Thus, what was once a tedious search process that incurred exponentially more compute for each task added can now be accomplished within a few training runs, irrespective of the number of tasks. Ultimately, we will demonstrate that gradient manipulation affords us great control over the training dynamics of multitask networks and may be one of the keys to unlocking the potential of multitask learning.},
	urldate = {2021-05-27},
	journal = {arXiv:1711.02257 [cs]},
	author = {Chen, Zhao and Badrinarayanan, Vijay and Lee, Chen-Yu and Rabinovich, Andrew},
	month = jun,
	year = {2018},
	note = {arXiv: 1711.02257},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: ICML 2018},
	file = {arXiv Fulltext PDF:/Users/nielswarncke/Zotero/storage/EIUXIB68/Chen et al. - 2018 - GradNorm Gradient Normalization for Adaptive Loss.pdf:application/pdf;arXiv.org Snapshot:/Users/nielswarncke/Zotero/storage/B7VKR8Q7/1711.html:text/html},
}

@article{wang_neural_2019,
	title = {Neural source-filter waveform models for statistical parametric speech synthesis},
	url = {http://arxiv.org/abs/1904.12088},
	abstract = {Neural waveform models such as WaveNet have demonstrated better performance than conventional vocoders for statistical parametric speech synthesis. As an autoregressive (AR) model, WaveNet is limited by a slow sequential waveform generation process. Some new models that use the inverse-autoregressive flow (IAF) can generate a whole waveform in a one-shot manner. However, these IAF-based models require sequential transformation during training, which severely slows down the training speed. Other models such as Parallel WaveNet and ClariNet bring together the benefits of AR and IAF-based models and train an IAF model by transferring the knowledge from a pre-trained AR teacher to an IAF student without any sequential transformation. However, both models require additional training criteria, and their implementation is prohibitively complicated. We propose a framework for neural source-filter (NSF) waveform modeling without AR nor IAF-based approaches. This framework requires only three components for waveform generation: a source module that generates a sine-based signal as excitation, a non-AR dilated-convolution-based filter module that transforms the excitation into a waveform, and a conditional module that pre-processes the acoustic features for the source and filer modules. This framework minimizes spectral-amplitude distances for model training, which can be efficiently implemented by using short-time Fourier transform routines. Under this framework, we designed three NSF models and compared them with WaveNet. It was demonstrated that the NSF models generated waveforms at least 100 times faster than WaveNet, and the quality of the synthetic speech from the best NSF model was better than or equally good as that from WaveNet.},
	urldate = {2021-07-22},
	journal = {arXiv:1904.12088 [cs, eess, stat]},
	author = {Wang, Xin and Takaki, Shinji and Yamagishi, Junichi},
	month = nov,
	year = {2019},
	note = {arXiv: 1904.12088},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning},
	annote = {Comment: Accepted to IEEE/ACM TASLP. Note: this paper is on a follow-up work of our ICASSP paper. Based on the h-NSF introduced in this work, we proposed a h-sinc-NSF model and published the third paper in SSW 10 (https://www.isca-speech.org/archive/SSW\_2019/pdfs/SSW10\_O\_1-1.pdf)},
	file = {arXiv Fulltext PDF:/Users/nielswarncke/Zotero/storage/TTGANEPD/Wang et al. - 2019 - Neural source-filter waveform models for statistic.pdf:application/pdf;arXiv.org Snapshot:/Users/nielswarncke/Zotero/storage/V2ASXIIE/1904.html:text/html},
}

@article{hayes_neural_nodate,
	title = {{NEURAL} {WAVESHAPING} {SYNTHESIS}},
	abstract = {We present the Neural Waveshaping Unit (NEWT): a novel, lightweight, fully causal approach to neural audio synthesis which operates directly in the waveform domain, with an accompanying optimisation (FastNEWT) for efﬁcient CPU inference. The NEWT uses time-distributed multilayer perceptrons with periodic activations to implicitly learn nonlinear transfer functions that encode the characteristics of a target timbre. Once trained, a NEWT can produce complex timbral evolutions by simple afﬁne transformations of its input and output signals. We paired the NEWT with a differentiable noise synthesiser and reverb and found it capable of generating realistic musical instrument performances with only 260k total model parameters, conditioned on F0 and loudness features. We compared our method to state-of-the-art benchmarks with a multistimulus listening test and the Fréchet Audio Distance and found it performed competitively across the tested timbral domains. Our method signiﬁcantly outperformed the benchmarks in terms of generation speed, and achieved real-time performance on a consumer CPU, both with and without FastNEWT, suggesting it is a viable basis for future creative sound design tools.},
	language = {en},
	author = {Hayes, Ben and Saitis, Charalampos and Fazekas, George},
	pages = {8},
	file = {Hayes et al. - NEURAL WAVESHAPING SYNTHESIS.pdf:/Users/nielswarncke/Zotero/storage/574ZC3GP/Hayes et al. - NEURAL WAVESHAPING SYNTHESIS.pdf:application/pdf},
}

@inproceedings{kalchbrenner_efficient_2018,
	title = {Efficient {Neural} {Audio} {Synthesis}},
	url = {http://proceedings.mlr.press/v80/kalchbrenner18a.html},
	language = {en},
	urldate = {2021-07-23},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Kalchbrenner, Nal and Elsen, Erich and Simonyan, Karen and Noury, Seb and Casagrande, Norman and Lockhart, Edward and Stimberg, Florian and Oord, Aaron and Dieleman, Sander and Kavukcuoglu, Koray},
	month = jul,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {2410--2419},
	file = {Full Text PDF:/Users/nielswarncke/Zotero/storage/8HZLXI7F/Kalchbrenner et al. - 2018 - Efficient Neural Audio Synthesis.pdf:application/pdf},
}

@article{kong_diffwave_2021,
	title = {{DiffWave}: {A} {Versatile} {Diffusion} {Model} for {Audio} {Synthesis}},
	shorttitle = {{DiffWave}},
	url = {http://arxiv.org/abs/2009.09761},
	abstract = {In this work, we propose DiffWave, a versatile diffusion probabilistic model for conditional and unconditional waveform generation. The model is non-autoregressive, and converts the white noise signal into structured waveform through a Markov chain with a constant number of steps at synthesis. It is efficiently trained by optimizing a variant of variational bound on the data likelihood. DiffWave produces high-fidelity audios in different waveform generation tasks, including neural vocoding conditioned on mel spectrogram, class-conditional generation, and unconditional generation. We demonstrate that DiffWave matches a strong WaveNet vocoder in terms of speech quality (MOS: 4.44 versus 4.43), while synthesizing orders of magnitude faster. In particular, it significantly outperforms autoregressive and GAN-based waveform models in the challenging unconditional generation task in terms of audio quality and sample diversity from various automatic and human evaluations.},
	urldate = {2021-07-23},
	journal = {arXiv:2009.09761 [cs, eess, stat]},
	author = {Kong, Zhifeng and Ping, Wei and Huang, Jiaji and Zhao, Kexin and Catanzaro, Bryan},
	month = mar,
	year = {2021},
	note = {arXiv: 2009.09761},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning, Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: ICLR 2021 (oral)},
	file = {arXiv Fulltext PDF:/Users/nielswarncke/Zotero/storage/CY8ADAY2/Kong et al. - 2021 - DiffWave A Versatile Diffusion Model for Audio Sy.pdf:application/pdf;arXiv.org Snapshot:/Users/nielswarncke/Zotero/storage/FEJG8G8Y/2009.html:text/html},
}

@inproceedings{prenger_waveglow_2019,
	title = {Waveglow: {A} {Flow}-based {Generative} {Network} for {Speech} {Synthesis}},
	shorttitle = {Waveglow},
	doi = {10.1109/ICASSP.2019.8683143},
	abstract = {In this paper we propose WaveGlow: a flow-based network capable of generating high quality speech from mel-spectrograms. WaveGlow combines insights from Glow [1] and WaveNet [2] in order to provide fast, efficient and high-quality audio synthesis, without the need for auto-regression. WaveGlow is implemented using only a single network, trained using only a single cost function: maximizing the likelihood of the training data, which makes the training procedure simple and stable. Our PyTorch implementation produces audio samples at a rate of more than 500 kHz on an NVIDIA V100 GPU. Mean Opinion Scores show that it delivers audio quality as good as the best publicly available WaveNet implementation. All code will be made publicly available online [3].},
	booktitle = {{ICASSP} 2019 - 2019 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Prenger, Ryan and Valle, Rafael and Catanzaro, Bryan},
	month = may,
	year = {2019},
	note = {ISSN: 2379-190X},
	keywords = {Audio Synthesis, Deep Learning, Generative models, Text-to-speech},
	pages = {3617--3621},
	file = {Submitted Version:/Users/nielswarncke/Zotero/storage/9SPER9LM/Prenger et al. - 2019 - Waveglow A Flow-based Generative Network for Spee.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/nielswarncke/Zotero/storage/BJZ4C9DJ/8683143.html:text/html},
}

@article{donahue_adversarial_2019,
	title = {Adversarial {Audio} {Synthesis}},
	url = {http://arxiv.org/abs/1802.04208},
	abstract = {Audio signals are sampled at high temporal resolutions, and learning to synthesize audio requires capturing structure across a range of timescales. Generative adversarial networks (GANs) have seen wide success at generating images that are both locally and globally coherent, but they have seen little application to audio generation. In this paper we introduce WaveGAN, a first attempt at applying GANs to unsupervised synthesis of raw-waveform audio. WaveGAN is capable of synthesizing one second slices of audio waveforms with global coherence, suitable for sound effect generation. Our experiments demonstrate that, without labels, WaveGAN learns to produce intelligible words when trained on a small-vocabulary speech dataset, and can also synthesize audio from other domains such as drums, bird vocalizations, and piano. We compare WaveGAN to a method which applies GANs designed for image generation on image-like audio feature representations, finding both approaches to be promising.},
	urldate = {2021-07-23},
	journal = {arXiv:1802.04208 [cs]},
	author = {Donahue, Chris and McAuley, Julian and Puckette, Miller},
	month = feb,
	year = {2019},
	note = {arXiv: 1802.04208
version: 3},
	keywords = {Computer Science - Sound, Computer Science - Machine Learning},
	annote = {Comment: Published as a conference paper at ICLR 2019},
	file = {arXiv Fulltext PDF:/Users/nielswarncke/Zotero/storage/YYH9CG8R/Donahue et al. - 2019 - Adversarial Audio Synthesis.pdf:application/pdf;arXiv.org Snapshot:/Users/nielswarncke/Zotero/storage/B3A4UGH4/1802.html:text/html},
}

@article{defossez_music_2021,
	title = {Music {Source} {Separation} in the {Waveform} {Domain}},
	url = {http://arxiv.org/abs/1911.13254},
	abstract = {Source separation for music is the task of isolating contributions, or stems, from different instruments recorded individually and arranged together to form a song. Such components include voice, bass, drums and any other accompaniments.Contrarily to many audio synthesis tasks where the best performances are achieved by models that directly generate the waveform, the state-of-the-art in source separation for music is to compute masks on the magnitude spectrum. In this paper, we compare two waveform domain architectures. We first adapt Conv-Tasnet, initially developed for speech source separation,to the task of music source separation. While Conv-Tasnet beats many existing spectrogram-domain methods, it suffersfrom significant artifacts, as shown by human evaluations. We propose instead Demucs, a novel waveform-to-waveform model,with a U-Net structure and bidirectional LSTM.Experiments on the MusDB dataset show that, with proper data augmentation, Demucs beats allexisting state-of-the-art architectures, including Conv-Tasnet, with 6.3 SDR on average, (and up to 6.8 with 150 extra training songs, even surpassing the IRM oracle for the bass source).Using recent development in model quantization, Demucs can be compressed down to 120MBwithout any loss of accuracy.We also provide human evaluations, showing that Demucs benefit from a large advantagein terms of the naturalness of the audio. However, it suffers from some bleeding,especially between the vocals and other source.},
	urldate = {2021-07-23},
	journal = {arXiv:1911.13254 [cs, eess, stat]},
	author = {Défossez, Alexandre and Usunier, Nicolas and Bottou, Léon and Bach, Francis},
	month = apr,
	year = {2021},
	note = {arXiv: 1911.13254
version: 2},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/nielswarncke/Zotero/storage/W7A6QQVV/Défossez et al. - 2021 - Music Source Separation in the Waveform Domain.pdf:application/pdf;arXiv.org Snapshot:/Users/nielswarncke/Zotero/storage/JR7FYH24/1911.html:text/html},
}

@article{oord_parallel_2017,
	title = {Parallel {WaveNet}: {Fast} {High}-{Fidelity} {Speech} {Synthesis}},
	shorttitle = {Parallel {WaveNet}},
	url = {http://arxiv.org/abs/1711.10433},
	abstract = {The recently-developed WaveNet architecture is the current state of the art in realistic speech synthesis, consistently rated as more natural sounding for many different languages than any previous system. However, because WaveNet relies on sequential generation of one audio sample at a time, it is poorly suited to today's massively parallel computers, and therefore hard to deploy in a real-time production setting. This paper introduces Probability Density Distillation, a new method for training a parallel feed-forward network from a trained WaveNet with no significant difference in quality. The resulting system is capable of generating high-fidelity speech samples at more than 20 times faster than real-time, and is deployed online by Google Assistant, including serving multiple English and Japanese voices.},
	urldate = {2021-07-23},
	journal = {arXiv:1711.10433 [cs]},
	author = {Oord, Aaron van den and Li, Yazhe and Babuschkin, Igor and Simonyan, Karen and Vinyals, Oriol and Kavukcuoglu, Koray and Driessche, George van den and Lockhart, Edward and Cobo, Luis C. and Stimberg, Florian and Casagrande, Norman and Grewe, Dominik and Noury, Seb and Dieleman, Sander and Elsen, Erich and Kalchbrenner, Nal and Zen, Heiga and Graves, Alex and King, Helen and Walters, Tom and Belov, Dan and Hassabis, Demis},
	month = nov,
	year = {2017},
	note = {arXiv: 1711.10433},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/nielswarncke/Zotero/storage/TBEMZD42/Oord et al. - 2017 - Parallel WaveNet Fast High-Fidelity Speech Synthe.pdf:application/pdf;arXiv.org Snapshot:/Users/nielswarncke/Zotero/storage/NZTGHPED/1711.html:text/html},
}

@article{engel_gansynth_2019,
	title = {{GANSynth}: {Adversarial} {Neural} {Audio} {Synthesis}},
	shorttitle = {{GANSynth}},
	url = {http://arxiv.org/abs/1902.08710},
	abstract = {Efficient audio synthesis is an inherently difficult machine learning task, as human perception is sensitive to both global structure and fine-scale waveform coherence. Autoregressive models, such as WaveNet, model local structure at the expense of global latent structure and slow iterative sampling, while Generative Adversarial Networks (GANs), have global latent conditioning and efficient parallel sampling, but struggle to generate locally-coherent audio waveforms. Herein, we demonstrate that GANs can in fact generate high-fidelity and locally-coherent audio by modeling log magnitudes and instantaneous frequencies with sufficient frequency resolution in the spectral domain. Through extensive empirical investigations on the NSynth dataset, we demonstrate that GANs are able to outperform strong WaveNet baselines on automated and human evaluation metrics, and efficiently generate audio several orders of magnitude faster than their autoregressive counterparts.},
	urldate = {2021-07-23},
	journal = {arXiv:1902.08710 [cs, eess, stat]},
	author = {Engel, Jesse and Agrawal, Kumar Krishna and Chen, Shuo and Gulrajani, Ishaan and Donahue, Chris and Roberts, Adam},
	month = apr,
	year = {2019},
	note = {arXiv: 1902.08710},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning, Computer Science - Machine Learning},
	annote = {Comment: Colab Notebook: http://goo.gl/magenta/gansynth-demo},
	file = {arXiv Fulltext PDF:/Users/nielswarncke/Zotero/storage/JP9VV6K2/Engel et al. - 2019 - GANSynth Adversarial Neural Audio Synthesis.pdf:application/pdf;arXiv.org Snapshot:/Users/nielswarncke/Zotero/storage/S9X99PSH/1902.html:text/html},
}

@article{hantrakul_fast_2019,
	title = {{FAST} {AND} {FLEXIBLE} {NEURAL} {AUDIO} {SYNTHESIS}},
	language = {en},
	author = {Hantrakul, Lamtharn and Engel, Jesse and Roberts, Adam and Gu, Chenjie},
	year = {2019},
	pages = {7},
	file = {Hantrakul et al. - 2019 - FAST AND FLEXIBLE NEURAL AUDIO SYNTHESIS.pdf:/Users/nielswarncke/Zotero/storage/MTPX9R7Y/Hantrakul et al. - 2019 - FAST AND FLEXIBLE NEURAL AUDIO SYNTHESIS.pdf:application/pdf},
}

@article{hantrakul_fast_2019-1,
	title = {{FAST} {AND} {FLEXIBLE} {NEURAL} {AUDIO} {SYNTHESIS}},
	language = {en},
	author = {Hantrakul, Lamtharn and Engel, Jesse and Roberts, Adam and Gu, Chenjie},
	year = {2019},
	pages = {7},
	file = {Hantrakul et al. - 2019 - FAST AND FLEXIBLE NEURAL AUDIO SYNTHESIS.pdf:/Users/nielswarncke/Zotero/storage/GTB986ZC/Hantrakul et al. - 2019 - FAST AND FLEXIBLE NEURAL AUDIO SYNTHESIS.pdf:application/pdf},
}

@article{chung_empirical_2014,
	title = {Empirical {Evaluation} of {Gated} {Recurrent} {Neural} {Networks} on {Sequence} {Modeling}},
	url = {http://arxiv.org/abs/1412.3555},
	abstract = {In this paper we compare different types of recurrent units in recurrent neural networks (RNNs). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found GRU to be comparable to LSTM.},
	urldate = {2021-07-23},
	journal = {arXiv:1412.3555 [cs]},
	author = {Chung, Junyoung and Gulcehre, Caglar and Cho, KyungHyun and Bengio, Yoshua},
	month = dec,
	year = {2014},
	note = {arXiv: 1412.3555},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: Presented in NIPS 2014 Deep Learning and Representation Learning Workshop},
	file = {arXiv Fulltext PDF:/Users/nielswarncke/Zotero/storage/ELWJ6CCG/Chung et al. - 2014 - Empirical Evaluation of Gated Recurrent Neural Net.pdf:application/pdf;arXiv.org Snapshot:/Users/nielswarncke/Zotero/storage/TFCIXCJQ/1412.html:text/html},
}

@article{wang_deep_2019,
	title = {Deep {Text}-to-{Speech} {System} with {Seq2Seq} {Model}},
	url = {http://arxiv.org/abs/1903.07398},
	abstract = {Recent trends in neural network based text-to-speech/speech synthesis pipelines have employed recurrent Seq2seq architectures that can synthesize realistic sounding speech directly from text characters. These systems however have complex architectures and takes a substantial amount of time to train. We introduce several modifications to these Seq2seq architectures that allow for faster training time, and also allows us to reduce the complexity of the model architecture at the same time. We show that our proposed model can achieve attention alignment much faster than previous architectures and that good audio quality can be achieved with a model that's much smaller in size. Sample audio available at https://soundcloud.com/gary-wang-23/sets/tts-samples-for-cmpt-419.},
	urldate = {2021-08-13},
	journal = {arXiv:1903.07398 [cs]},
	author = {Wang, Gary},
	month = mar,
	year = {2019},
	note = {arXiv: 1903.07398},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/nielswarncke/Zotero/storage/UARZ5HM9/Wang - 2019 - Deep Text-to-Speech System with Seq2Seq Model.pdf:application/pdf;arXiv.org Snapshot:/Users/nielswarncke/Zotero/storage/CTWT45ZU/1903.html:text/html},
}

@article{dhariwal_diffusion_2021,
	title = {Diffusion {Models} {Beat} {GANs} on {Image} {Synthesis}},
	url = {http://arxiv.org/abs/2105.05233},
	abstract = {We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128\${\textbackslash}times\$128, 4.59 on ImageNet 256\${\textbackslash}times\$256, and 7.72 on ImageNet 512\${\textbackslash}times\$512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256\${\textbackslash}times\$256 and 3.85 on ImageNet 512\${\textbackslash}times\$512. We release our code at https://github.com/openai/guided-diffusion},
	urldate = {2021-08-13},
	journal = {arXiv:2105.05233 [cs, stat]},
	author = {Dhariwal, Prafulla and Nichol, Alex},
	month = jun,
	year = {2021},
	note = {arXiv: 2105.05233},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	annote = {Comment: Added compute requirements, ImageNet 256\${\textbackslash}times\$256 upsampling FID and samples, DDIM guided sampler, fixed typos},
	file = {arXiv Fulltext PDF:/Users/nielswarncke/Zotero/storage/XZK7FW54/Dhariwal and Nichol - 2021 - Diffusion Models Beat GANs on Image Synthesis.pdf:application/pdf;arXiv.org Snapshot:/Users/nielswarncke/Zotero/storage/7Y5DA94D/2105.html:text/html},
}

@inproceedings{liu_neural_2020,
	title = {Neural {Homomorphic} {Vocoder}},
	url = {http://www.isca-speech.org/archive/Interspeech_2020/abstracts/3188.html},
	doi = {10.21437/Interspeech.2020-3188},
	abstract = {In this paper, we propose the neural homomorphic vocoder (NHV), a source-ﬁlter model based neural vocoder framework. NHV synthesizes speech by ﬁltering impulse trains and noise with linear time-varying (LTV) ﬁlters. A neural network controls the LTV ﬁlters by estimating complex cepstrums of time-varying impulse responses given acoustic features. The proposed framework can be trained with a combination of multi-resolution STFT loss and adversarial loss functions. Due to the use of DSP-based synthesis methods, NHV is highly efﬁcient, fully controllable and interpretable. A vocoder was built under the framework to synthesize speech given log-Mel spectrograms and fundamental frequencies. While the model cost only 15 kFLOPs per sample, the synthesis quality remained comparable to baseline neural vocoders in both copy-synthesis and text-to-speech.},
	language = {en},
	urldate = {2021-08-14},
	booktitle = {Interspeech 2020},
	publisher = {ISCA},
	author = {Liu, Zhijun and Chen, Kuan and Yu, Kai},
	month = oct,
	year = {2020},
	pages = {240--244},
	file = {Liu et al. - 2020 - Neural Homomorphic Vocoder.pdf:/Users/nielswarncke/Zotero/storage/92EEMSU3/Liu et al. - 2020 - Neural Homomorphic Vocoder.pdf:application/pdf},
}

@article{mccarthy_hooligan_2020,
	title = {{HooliGAN}: {Robust}, {High} {Quality} {Neural} {Vocoding}},
	shorttitle = {{HooliGAN}},
	url = {http://arxiv.org/abs/2008.02493},
	abstract = {Recent developments in generative models have shown that deep learning combined with traditional digital signal processing (DSP) techniques could successfully generate convincing violin samples [1], that source-excitation combined with WaveNet yields high-quality vocoders [2, 3] and that generative adversarial network (GAN) training can improve naturalness [4, 5]. By combining the ideas in these models we introduce HooliGAN, a robust vocoder that has state of the art results, finetunes very well to smaller datasets ({\textless}30 minutes of speechdata) and generates audio at 2.2MHz on GPU and 35kHz on CPU. We also show a simple modification to Tacotron-basedmodels that allows seamless integration with HooliGAN. Results from our listening tests show the proposed model's ability to consistently output high-quality audio with a variety of datasets, big and small. We provide samples at the following demo page: https://resemble-ai.github.io/hooligan\_demo/},
	urldate = {2021-08-14},
	journal = {arXiv:2008.02493 [cs, eess]},
	author = {McCarthy, Ollie and Ahmed, Zohaib},
	month = aug,
	year = {2020},
	note = {arXiv: 2008.02493},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {arXiv Fulltext PDF:/Users/nielswarncke/Zotero/storage/G23MEBGD/McCarthy and Ahmed - 2020 - HooliGAN Robust, High Quality Neural Vocoding.pdf:application/pdf;arXiv.org Snapshot:/Users/nielswarncke/Zotero/storage/D2YUCXRP/2008.html:text/html},
}

@article{li_creating_2019,
	title = {Creating a {Multitrack} {Classical} {Music} {Performance} {Dataset} for {Multimodal} {Music} {Analysis}: {Challenges}, {Insights}, and {Applications}},
	volume = {21},
	issn = {1520-9210, 1941-0077},
	shorttitle = {Creating a {Multitrack} {Classical} {Music} {Performance} {Dataset} for {Multimodal} {Music} {Analysis}},
	url = {https://ieeexplore.ieee.org/document/8411155/},
	doi = {10.1109/TMM.2018.2856090},
	abstract = {We introduce a dataset for facilitating audio-visual analysis of music performances. The dataset comprises 44 simple multi-instrument classical music pieces assembled from coordinated but separately recorded performances of individual tracks. For each piece, we provide the musical score in MIDI format, the audio recordings of the individual tracks, the audio and video recording of the assembled mixture, and ground-truth annotation ﬁles including frame-level and note-level transcriptions. We describe our methodology for the creation of the dataset, particularly highlighting our approaches for addressing the challenges involved in maintaining synchronization and expressiveness. We demonstrate the high quality of synchronization achieved with our proposed approach by comparing the dataset with existing widely-used music audio datasets.},
	language = {en},
	number = {2},
	urldate = {2021-08-16},
	journal = {IEEE Transactions on Multimedia},
	author = {Li, Bochen and Liu, Xinzhao and Dinesh, Karthik and Duan, Zhiyao and Sharma, Gaurav},
	month = feb,
	year = {2019},
	pages = {522--535},
	file = {Li et al. - 2019 - Creating a Multitrack Classical Music Performance .pdf:/Users/nielswarncke/Zotero/storage/YURZZNEW/Li et al. - 2019 - Creating a Multitrack Classical Music Performance .pdf:application/pdf},
}

@article{turian_im_nodate,
	title = {I’m {Sorry} for {Your} {Loss}: {Spectrally}-{Based} {Audio} {Distances} {Are} {Bad} at {Pitch}},
	abstract = {Growing research demonstrates that synthetic failure modes imply poor generalization. We compare commonly used audio-to-audio losses on a synthetic benchmark, measuring the pitch distance between two stationary sinusoids. The results are surprising: many have poor sense of pitch direction. These shortcomings are exposed using simple rank assumptions. Our task is trivial for humans but difﬁcult for these audio distances, suggesting signiﬁcant progress can be made in self-supervised audio learning by improving current losses.},
	language = {en},
	author = {Turian, Joseph and Henry, Max},
	pages = {16},
	file = {Turian and Henry - I’m Sorry for Your Loss Spectrally-Based Audio Di.pdf:/Users/nielswarncke/Zotero/storage/56C4MCJV/Turian and Henry - I’m Sorry for Your Loss Spectrally-Based Audio Di.pdf:application/pdf},
}

@article{roberts_hierarchical_2019,
	title = {A {Hierarchical} {Latent} {Vector} {Model} for {Learning} {Long}-{Term} {Structure} in {Music}},
	url = {http://arxiv.org/abs/1803.05428},
	abstract = {The Variational Autoencoder (VAE) has proven to be an effective model for producing semantically meaningful latent representations for natural data. However, it has thus far seen limited application to sequential data, and, as we demonstrate, existing recurrent VAE models have difficulty modeling sequences with long-term structure. To address this issue, we propose the use of a hierarchical decoder, which first outputs embeddings for subsequences of the input and then uses these embeddings to generate each subsequence independently. This structure encourages the model to utilize its latent code, thereby avoiding the "posterior collapse" problem, which remains an issue for recurrent VAEs. We apply this architecture to modeling sequences of musical notes and find that it exhibits dramatically better sampling, interpolation, and reconstruction performance than a "flat" baseline model. An implementation of our "MusicVAE" is available online at http://g.co/magenta/musicvae-code.},
	urldate = {2021-09-03},
	journal = {arXiv:1803.05428 [cs, eess, stat]},
	author = {Roberts, Adam and Engel, Jesse and Raffel, Colin and Hawthorne, Curtis and Eck, Douglas},
	month = nov,
	year = {2019},
	note = {arXiv: 1803.05428},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning},
	annote = {Comment: ICML Camera Ready Version (w/ fixed typos)},
	file = {arXiv Fulltext PDF:/Users/nielswarncke/Zotero/storage/WFGYCJD8/Roberts et al. - 2019 - A Hierarchical Latent Vector Model for Learning Lo.pdf:application/pdf;arXiv.org Snapshot:/Users/nielswarncke/Zotero/storage/4ZIUISRB/1803.html:text/html},
}

@article{kim_crepe_2018,
	title = {{CREPE}: {A} {Convolutional} {Representation} for {Pitch} {Estimation}},
	shorttitle = {{CREPE}},
	url = {http://arxiv.org/abs/1802.06182},
	abstract = {The task of estimating the fundamental frequency of a monophonic sound recording, also known as pitch tracking, is fundamental to audio processing with multiple applications in speech processing and music information retrieval. To date, the best performing techniques, such as the pYIN algorithm, are based on a combination of DSP pipelines and heuristics. While such techniques perform very well on average, there remain many cases in which they fail to correctly estimate the pitch. In this paper, we propose a data-driven pitch tracking algorithm, CREPE, which is based on a deep convolutional neural network that operates directly on the time-domain waveform. We show that the proposed model produces state-of-the-art results, performing equally or better than pYIN. Furthermore, we evaluate the model's generalizability in terms of noise robustness. A pre-trained version of CREPE is made freely available as an open-source Python module for easy application.},
	urldate = {2021-09-03},
	journal = {arXiv:1802.06182 [cs, eess, stat]},
	author = {Kim, Jong Wook and Salamon, Justin and Li, Peter and Bello, Juan Pablo},
	month = feb,
	year = {2018},
	note = {arXiv: 1802.06182},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning},
	annote = {Comment: ICASSP 2018},
	file = {arXiv Fulltext PDF:/Users/nielswarncke/Zotero/storage/H5J6A2GP/Kim et al. - 2018 - CREPE A Convolutional Representation for Pitch Es.pdf:application/pdf;arXiv.org Snapshot:/Users/nielswarncke/Zotero/storage/YL9KQ5NM/1802.html:text/html},
}
