
@misc{noauthor_learning_nodate,
	title = {Learning to write programs that generate images},
	url = {/blog/article/learning-to-generate-images},
	abstract = {Through a human’s eyes, the world is much more than just the images reflected in our corneas. For example, when we look at a building and admire the intricacies of its design, we can appreciate the craftsmanship it requires. This ability to interpret objects through the tools that created them gives us a richer understanding of the world and is an important aspect of our intelligence.We would like our systems to create similarly rich representations of the world. For example, when observing an image of a painting we would like them to understand the brush strokes used to create it and not just the pixels that represent it on a screen.In this work, we equipped artificial agents with the same tools that we use to generate images and demonstrate that they can reason about how digits, characters and portraits are constructed. Crucially, they learn to do this by themselves and without the need for human-labelled datasets. This contrasts with recent research which has so far relied on learning from human demonstrations, which can be a time-intensive process.},
	language = {ALL},
	urldate = {2020-10-15},
	journal = {Deepmind},
	file = {Snapshot:/Users/p378593/Zotero/storage/I8SZ828Y/learning-to-generate-images.html:text/html}
}

@article{generative-rl,
	title = {Synthesizing {Programs} for {Images} using {Reinforced} {Adversarial} {Learning}},
	abstract = {Advances in deep generative networks have led to impressive results in recent years. Nevertheless, such models can often waste their capacity on the minutiae of datasets, presumably due to weak inductive biases in their decoders. This is where graphics engines may come in handy since they abstract away low-level details and represent images as high-level programs. Current methods that combine deep learning and renderers are limited by hand-crafted likelihood or distance functions, a need for large amounts of supervision, or difﬁculties in scaling their inference algorithms to richer datasets. To mitigate these issues, we present SPIRAL, an adversarially trained agent that generates a program which is executed by a graphics engine to interpret and sample images. The goal of this agent is to fool a discriminator network that distinguishes between real and rendered data, trained with a distributed reinforcement learning setup without any supervision. A surprising ﬁnding is that using the discriminator’s output as a reward signal is the key to allow the agent to make meaningful progress at matching the desired output rendering. To the best of our knowledge, this is the ﬁrst demonstration of an end-to-end, unsupervised and adversarial inverse graphics agent on challenging real world (MNIST, OMNIGLOT, CELEBA) and synthetic 3D datasets. A video of the agent can be found at https://youtu.be/iSyvwAwa7vk.},
	language = {en},
	author = {Ganin, Yaroslav and Kulkarni, Tejas and Babuschkin, Igor and Eslami, S M Ali and Vinyals, Oriol},
	pages = {12},
	file = {Ganin et al. - Synthesizing Programs for Images using Reinforced .pdf:/Users/p378593/Zotero/storage/5466L9ZU/Ganin et al. - Synthesizing Programs for Images using Reinforced .pdf:application/pdf}
}

@article{ddsp,
	title = {{DIFFERENTIABLE} {DIGITAL} {SIGNAL} {PROCESSING}},
	abstract = {Most generative models of audio directly generate samples in one of two domains: time or frequency. While sufﬁcient to express any signal, these representations are inefﬁcient, as they do not utilize existing knowledge of how sound is generated and perceived. A third approach (vocoders/synthesizers) successfully incorporates strong domain knowledge of signal processing and perception, but has been less actively researched due to limited expressivity and difﬁculty integrating with modern auto-differentiation-based machine learning methods. In this paper, we introduce the Differentiable Digital Signal Processing (DDSP) library, which enables direct integration of classic signal processing elements with deep learning methods. Focusing on audio synthesis, we achieve high-ﬁdelity generation without the need for large autoregressive models or adversarial losses, demonstrating that DDSP enables utilizing strong inductive biases without losing the expressive power of neural networks. Further, we show that combining interpretable modules permits manipulation of each separate model component, with applications such as independent control of pitch and loudness, realistic extrapolation to pitches not seen during training, blind dereverberation of room acoustics, transfer of extracted room acoustics to new environments, and transformation of timbre between disparate sources. In short, DDSP enables an interpretable and modular approach to generative modeling, without sacriﬁcing the beneﬁts of deep learning. The library is publicly available1 and we welcome further contributions from the community and domain experts.},
	language = {en},
	author = {Engel, Jesse and Hantrakul, Lamtharn and Gu, Chenjie and Roberts, Adam},
	year = {2020},
	pages = {19},
	file = {Engel et al. - 2020 - DIFFERENTIABLE DIGITAL SIGNAL PROCESSING.pdf:/Users/p378593/Zotero/storage/I7V4YACC/Engel et al. - 2020 - DIFFERENTIABLE DIGITAL SIGNAL PROCESSING.pdf:application/pdf}
}

@article{karras_analyzing_2020,
	title = {Analyzing and {Improving} the {Image} {Quality} of {StyleGAN}},
	url = {http://arxiv.org/abs/1912.04958},
	abstract = {The style-based GAN architecture (StyleGAN) yields state-of-the-art results in data-driven unconditional generative image modeling. We expose and analyze several of its characteristic artifacts, and propose changes in both model architecture and training methods to address them. In particular, we redesign the generator normalization, revisit progressive growing, and regularize the generator to encourage good conditioning in the mapping from latent codes to images. In addition to improving image quality, this path length regularizer yields the additional benefit that the generator becomes significantly easier to invert. This makes it possible to reliably attribute a generated image to a particular network. We furthermore visualize how well the generator utilizes its output resolution, and identify a capacity problem, motivating us to train larger models for additional quality improvements. Overall, our improved model redefines the state of the art in unconditional image modeling, both in terms of existing distribution quality metrics as well as perceived image quality.},
	urldate = {2020-10-15},
	journal = {arXiv:1912.04958 [cs, eess, stat]},
	author = {Karras, Tero and Laine, Samuli and Aittala, Miika and Hellsten, Janne and Lehtinen, Jaakko and Aila, Timo},
	month = mar,
	year = {2020},
	note = {arXiv: 1912.04958},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Electrical Engineering and Systems Science - Image and Video Processing, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/p378593/Zotero/storage/RMA8LQJC/Karras et al. - 2020 - Analyzing and Improving the Image Quality of Style.pdf:application/pdf;arXiv.org Snapshot:/Users/p378593/Zotero/storage/QZ7SPHCX/1912.html:text/html}
}

@article{gan-survey,
	title = {Recent {Progress} on {Generative} {Adversarial} {Networks} ({GANs}): {A} {Survey}},
	volume = {7},
	issn = {2169-3536},
	shorttitle = {Recent {Progress} on {Generative} {Adversarial} {Networks} ({GANs})},
	doi = {10.1109/ACCESS.2019.2905015},
	abstract = {Generative adversarial network (GANs) is one of the most important research avenues in the field of artificial intelligence, and its outstanding data generation capacity has received wide attention. In this paper, we present the recent progress on GANs. First, the basic theory of GANs and the differences among different generative models in recent years were analyzed and summarized. Then, the derived models of GANs are classified and introduced one by one. Third, the training tricks and evaluation metrics were given. Fourth, the applications of GANs were introduced. Finally, the problem, we need to address, and future directions were discussed.},
	journal = {IEEE Access},
	author = {Pan, Zhaoqing and Yu, Weijie and Yi, Xiaokai and Khan, Asifullah and Yuan, Feng and Zheng, Yuhui},
	year = {2019},
	note = {Conference Name: IEEE Access},
	keywords = {artificial intelligence, data generation capacity, Data models, Deep learning, Feature extraction, Gallium nitride, GANs, generative adversarial network, generative adversarial networks, Generative adversarial networks, generative models, Generators, machine learning, neural nets, Training, unsupervised learning, Unsupervised learning},
	pages = {36322--36333},
	file = {IEEE Xplore Full Text PDF:/Users/p378593/Zotero/storage/FKPI8TGF/Pan et al. - 2019 - Recent Progress on Generative Adversarial Networks.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/p378593/Zotero/storage/DUDM2JCV/8667290.html:text/html}
}

@article{jukebox,
	title = {Jukebox: {A} {Generative} {Model} for {Music}},
	shorttitle = {Jukebox},
	url = {http://arxiv.org/abs/2005.00341},
	abstract = {We introduce Jukebox, a model that generates music with singing in the raw audio domain. We tackle the long context of raw audio using a multi-scale VQ-VAE to compress it to discrete codes, and modeling those using autoregressive Transformers. We show that the combined model at scale can generate high-fidelity and diverse songs with coherence up to multiple minutes. We can condition on artist and genre to steer the musical and vocal style, and on unaligned lyrics to make the singing more controllable. We are releasing thousands of non cherry-picked samples at https://jukebox.openai.com, along with model weights and code at https://github.com/openai/jukebox},
	urldate = {2020-10-15},
	journal = {arXiv:2005.00341 [cs, eess, stat]},
	author = {Dhariwal, Prafulla and Jun, Heewoo and Payne, Christine and Kim, Jong Wook and Radford, Alec and Sutskever, Ilya},
	month = apr,
	year = {2020},
	note = {arXiv: 2005.00341},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {arXiv Fulltext PDF:/Users/p378593/Zotero/storage/SGK3S6IW/Dhariwal et al. - 2020 - Jukebox A Generative Model for Music.pdf:application/pdf;arXiv.org Snapshot:/Users/p378593/Zotero/storage/5FFKUFSF/2005.html:text/html}
}

@article{gon,
	title = {Gradient {Origin} {Networks}},
	url = {http://arxiv.org/abs/2007.02798},
	abstract = {This paper proposes a new type of generative model that is able to quickly learn a latent representation without an encoder. This is achieved by initialising a latent vector with zeros, then using gradients of the data fitting loss with respect to this zero vector as new latent points. The approach has similar characteristics to autoencoders but with a simpler naturally balanced architecture, and is demonstrated in a variational autoencoder equivalent that permits sampling. This also allows implicit representation networks to learn a space of implicit functions without requiring a hypernetwork, retaining their representation advantages with fewer parameters.},
	urldate = {2020-10-15},
	journal = {arXiv:2007.02798 [cs]},
	author = {Bond-Taylor, Sam and Willcocks, Chris G.},
	month = jul,
	year = {2020},
	note = {arXiv: 2007.02798},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, 68T01 (Primary), 68T07 (Secondary), G.3, I.4.0, I.5.0},
	annote = {Comment: 6 pages, 9 figures, generalised to non-implicit functions and added new experiments},
	file = {arXiv Fulltext PDF:/Users/p378593/Zotero/storage/D393YTEH/Bond-Taylor und Willcocks - 2020 - Gradient Origin Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/p378593/Zotero/storage/QSQ7UZDW/2007.html:text/html}
}

@article{wavenet,
	title = {{WaveNet}: {A} {Generative} {Model} for {Raw} {Audio}},
	shorttitle = {{WaveNet}},
	url = {http://arxiv.org/abs/1609.03499},
	abstract = {This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.},
	urldate = {2020-10-15},
	journal = {arXiv:1609.03499 [cs]},
	author = {Oord, Aaron van den and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
	month = sep,
	year = {2016},
	note = {arXiv: 1609.03499},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound},
	file = {arXiv Fulltext PDF:/Users/p378593/Zotero/storage/N239NY9B/Oord et al. - 2016 - WaveNet A Generative Model for Raw Audio.pdf:application/pdf;arXiv.org Snapshot:/Users/p378593/Zotero/storage/QSN7ZY4V/1609.html:text/html}
}

@article{engel_self-supervised_2020,
	title = {Self-supervised {Pitch} {Detection} by {Inverse} {Audio} {Synthesis}},
	url = {https://openreview.net/forum?id=RlVTYWhsky7&fbclid=IwAR15QmETdZ5d3DzpKPjfffYbN5Q6U12-Zj3mgWPiF2EKjKT90lYiXHBIZt4},
	abstract = {Disentangling pitch and timbre with inverse differentiable audio rendering},
	language = {en},
	urldate = {2020-10-15},
	author = {Engel, Jesse and Swavely, Rigel and Hantrakul, Lamtharn Hanoi and Roberts, Adam and Hawthorne, Curtis},
	month = jun,
	year = {2020},
	file = {Full Text PDF:/Users/p378593/Zotero/storage/GIX3TKLZ/Engel et al. - 2020 - Self-supervised Pitch Detection by Inverse Audio S.pdf:application/pdf;Snapshot:/Users/p378593/Zotero/storage/4X57IJJL/forum.html:text/html}
}

@article{siren,
	title = {Implicit {Neural} {Representations} with {Periodic} {Activation} {Functions}},
	url = {http://arxiv.org/abs/2006.09661},
	abstract = {Implicitly defined, continuous, differentiable signal representations parameterized by neural networks have emerged as a powerful paradigm, offering many possible benefits over conventional representations. However, current network architectures for such implicit neural representations are incapable of modeling signals with fine detail, and fail to represent a signal's spatial and temporal derivatives, despite the fact that these are essential to many physical signals defined implicitly as the solution to partial differential equations. We propose to leverage periodic activation functions for implicit neural representations and demonstrate that these networks, dubbed sinusoidal representation networks or Sirens, are ideally suited for representing complex natural signals and their derivatives. We analyze Siren activation statistics to propose a principled initialization scheme and demonstrate the representation of images, wavefields, video, sound, and their derivatives. Further, we show how Sirens can be leveraged to solve challenging boundary value problems, such as particular Eikonal equations (yielding signed distance functions), the Poisson equation, and the Helmholtz and wave equations. Lastly, we combine Sirens with hypernetworks to learn priors over the space of Siren functions.},
	urldate = {2020-10-15},
	journal = {arXiv:2006.09661 [cs, eess]},
	author = {Sitzmann, Vincent and Martel, Julien N. P. and Bergman, Alexander W. and Lindell, David B. and Wetzstein, Gordon},
	month = jun,
	year = {2020},
	note = {arXiv: 2006.09661},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing},
	annote = {Comment: Project website: https://vsitzmann.github.io/siren/ Project video: https://youtu.be/Q2fLWGBeaiI},
	file = {arXiv Fulltext PDF:/Users/p378593/Zotero/storage/9R28HYR5/Sitzmann et al. - 2020 - Implicit Neural Representations with Periodic Acti.pdf:application/pdf;arXiv.org Snapshot:/Users/p378593/Zotero/storage/27TGT8QJ/2006.html:text/html}
}

@article{vqvae2,
	title = {Generating {Diverse} {High}-{Fidelity} {Images} with {VQ}-{VAE}-2},
	url = {http://arxiv.org/abs/1906.00446},
	abstract = {We explore the use of Vector Quantized Variational AutoEncoder (VQ-VAE) models for large scale image generation. To this end, we scale and enhance the autoregressive priors used in VQ-VAE to generate synthetic samples of much higher coherence and fidelity than possible before. We use simple feed-forward encoder and decoder networks, making our model an attractive candidate for applications where the encoding and/or decoding speed is critical. Additionally, VQ-VAE requires sampling an autoregressive model only in the compressed latent space, which is an order of magnitude faster than sampling in the pixel space, especially for large images. We demonstrate that a multi-scale hierarchical organization of VQ-VAE, augmented with powerful priors over the latent codes, is able to generate samples with quality that rivals that of state of the art Generative Adversarial Networks on multifaceted datasets such as ImageNet, while not suffering from GAN's known shortcomings such as mode collapse and lack of diversity.},
	urldate = {2020-10-15},
	journal = {arXiv:1906.00446 [cs, stat]},
	author = {Razavi, Ali and Oord, Aaron van den and Vinyals, Oriol},
	month = jun,
	year = {2019},
	note = {arXiv: 1906.00446},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/p378593/Zotero/storage/RVD6AQZQ/Razavi et al. - 2019 - Generating Diverse High-Fidelity Images with VQ-VA.pdf:application/pdf;arXiv.org Snapshot:/Users/p378593/Zotero/storage/FF9RIK7J/1906.html:text/html}
}

@article{yamamoto_parallel_2020,
	title = {Parallel {WaveGAN}: {A} fast waveform generation model based on generative adversarial networks with multi-resolution spectrogram},
	shorttitle = {Parallel {WaveGAN}},
	url = {http://arxiv.org/abs/1910.11480},
	abstract = {We propose Parallel WaveGAN, a distillation-free, fast, and small-footprint waveform generation method using a generative adversarial network. In the proposed method, a non-autoregressive WaveNet is trained by jointly optimizing multi-resolution spectrogram and adversarial loss functions, which can effectively capture the time-frequency distribution of the realistic speech waveform. As our method does not require density distillation used in the conventional teacher-student framework, the entire model can be easily trained. Furthermore, our model is able to generate high-fidelity speech even with its compact architecture. In particular, the proposed Parallel WaveGAN has only 1.44 M parameters and can generate 24 kHz speech waveform 28.68 times faster than real-time on a single GPU environment. Perceptual listening test results verify that our proposed method achieves 4.16 mean opinion score within a Transformer-based text-to-speech framework, which is comparative to the best distillation-based Parallel WaveNet system.},
	urldate = {2020-10-15},
	journal = {arXiv:1910.11480 [cs, eess]},
	author = {Yamamoto, Ryuichi and Song, Eunwoo and Kim, Jae-Min},
	month = feb,
	year = {2020},
	note = {arXiv: 1910.11480},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Electrical Engineering and Systems Science - Signal Processing},
	annote = {Comment: Accepted to the conference of ICASSP 2020},
	file = {arXiv Fulltext PDF:/Users/p378593/Zotero/storage/FHVIK2NK/Yamamoto et al. - 2020 - Parallel WaveGAN A fast waveform generation model.pdf:application/pdf;arXiv.org Snapshot:/Users/p378593/Zotero/storage/N99MJTG8/1910.html:text/html}
}


@article{vqvae,
	title = {Neural {Discrete} {Representation} {Learning}},
	url = {http://arxiv.org/abs/1711.00937},
	abstract = {Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of "posterior collapse" -- where the latents are ignored when they are paired with a powerful autoregressive decoder -- typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.},
	urldate = {2020-10-16},
	journal = {arXiv:1711.00937 [cs]},
	author = {Oord, Aaron van den and Vinyals, Oriol and Kavukcuoglu, Koray},
	month = may,
	year = {2018},
	note = {arXiv: 1711.00937},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/p378593/Zotero/storage/2RGQTTNU/Oord et al. - 2018 - Neural Discrete Representation Learning.pdf:application/pdf;arXiv.org Snapshot:/Users/p378593/Zotero/storage/56XPNY56/1711.html:text/html}
}

@incollection{gan-original,
	title = {Generative {Adversarial} {Nets}},
	url = {http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf},
	urldate = {2020-10-20},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 27},
	publisher = {Curran Associates, Inc.},
	author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	editor = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. D. and Weinberger, K. Q.},
	year = {2014},
	pages = {2672--2680},
	file = {NIPS Full Text PDF:/Users/p378593/Zotero/storage/XDP94P8Q/Goodfellow et al. - 2014 - Generative Adversarial Nets.pdf:application/pdf;NIPS Snapshot:/Users/p378593/Zotero/storage/6RTTBAR3/5423-generative-adversarial-nets.html:text/html}
}

@article{wgan,
	title = {Wasserstein {GAN}},
	url = {http://arxiv.org/abs/1701.07875},
	abstract = {We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to other distances between distributions.},
	urldate = {2020-10-20},
	journal = {arXiv:1701.07875 [cs, stat]},
	author = {Arjovsky, Martin and Chintala, Soumith and Bottou, Léon},
	month = dec,
	year = {2017},
	note = {arXiv: 1701.07875},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/p378593/Zotero/storage/RLQZ8AGG/Arjovsky et al. - 2017 - Wasserstein GAN.pdf:application/pdf;arXiv.org Snapshot:/Users/p378593/Zotero/storage/REZT8JGX/1701.html:text/html}
}

@article{bert,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2020-10-20},
	journal = {arXiv:1810.04805 [cs]},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	note = {arXiv: 1810.04805},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/p378593/Zotero/storage/JI5S9KZP/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:application/pdf;arXiv.org Snapshot:/Users/p378593/Zotero/storage/KJHJC9A4/1810.html:text/html}
}

@article{gpt3,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	url = {http://arxiv.org/abs/2005.14165},
	abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
	urldate = {2020-10-20},
	journal = {arXiv:2005.14165 [cs]},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	month = jul,
	year = {2020},
	note = {arXiv: 2005.14165},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: 40+32 pages},
	file = {arXiv Fulltext PDF:/Users/p378593/Zotero/storage/G346IHM2/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf:application/pdf;arXiv.org Snapshot:/Users/p378593/Zotero/storage/HTS6QG4W/2005.html:text/html}
}

@article{attention,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	urldate = {2020-10-20},
	journal = {arXiv:1706.03762 [cs]},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = dec,
	year = {2017},
	note = {arXiv: 1706.03762},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	annote = {Comment: 15 pages, 5 figures},
	file = {arXiv Fulltext PDF:/Users/p378593/Zotero/storage/XJJ7TKQN/Vaswani et al. - 2017 - Attention Is All You Need.pdf:application/pdf;arXiv.org Snapshot:/Users/p378593/Zotero/storage/TNVT72S4/1706.html:text/html}
}

@article{music-transformer,
	title = {Music {Transformer}},
	url = {http://arxiv.org/abs/1809.04281},
	abstract = {Music relies heavily on repetition to build structure and meaning. Self-reference occurs on multiple timescales, from motifs to phrases to reusing of entire sections of music, such as in pieces with ABA structure. The Transformer (Vaswani et al., 2017), a sequence model based on self-attention, has achieved compelling results in many generation tasks that require maintaining long-range coherence. This suggests that self-attention might also be well-suited to modeling music. In musical composition and performance, however, relative timing is critically important. Existing approaches for representing relative positional information in the Transformer modulate attention based on pairwise distance (Shaw et al., 2018). This is impractical for long sequences such as musical compositions since their memory complexity for intermediate relative information is quadratic in the sequence length. We propose an algorithm that reduces their intermediate memory requirement to linear in the sequence length. This enables us to demonstrate that a Transformer with our modified relative attention mechanism can generate minute-long compositions (thousands of steps, four times the length modeled in Oore et al., 2018) with compelling structure, generate continuations that coherently elaborate on a given motif, and in a seq2seq setup generate accompaniments conditioned on melodies. We evaluate the Transformer with our relative attention mechanism on two datasets, JSB Chorales and Piano-e-Competition, and obtain state-of-the-art results on the latter.},
	urldate = {2020-10-26},
	journal = {arXiv:1809.04281 [cs, eess, stat]},
	author = {Huang, Cheng-Zhi Anna and Vaswani, Ashish and Uszkoreit, Jakob and Shazeer, Noam and Simon, Ian and Hawthorne, Curtis and Dai, Andrew M. and Hoffman, Matthew D. and Dinculescu, Monica and Eck, Douglas},
	month = dec,
	year = {2018},
	note = {arXiv: 1809.04281},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	annote = {Comment: Improved skewing section and accompanying figures. Previous titles are "An Improved Relative Self-Attention Mechanism for Transformer with Application to Music Generation" and "Music Transformer"},
	file = {arXiv Fulltext PDF:/Users/p378593/Zotero/storage/GGUMK73E/Huang et al. - 2018 - Music Transformer.pdf:application/pdf;arXiv.org Snapshot:/Users/p378593/Zotero/storage/F928GVVV/1809.html:text/html}
}

@inproceedings{anonymous_diffwave_2020,
	title = {{DiffWave}: {A} {Versatile} {Diffusion} {Model} for {Audio} {Synthesis}},
	shorttitle = {{DiffWave}},
	url = {https://openreview.net/forum?id=a-xFK8Ymz5J},
	abstract = {In this work, we propose DiffWave, a versatile Diffusion probabilistic model for conditional and unconditional Waveform generation. The model is non-autoregressive, and converts the white noise...},
	language = {en},
	urldate = {2020-11-11},
	author = {Anonymous},
	month = sep,
	year = {2020},
	file = {Full Text PDF:/Users/p378593/Zotero/storage/VAIW2X4I/Anonymous - 2020 - DiffWave A Versatile Diffusion Model for Audio Sy.pdf:application/pdf;Snapshot:/Users/p378593/Zotero/storage/PYHNZ2M4/forum.html:text/html}
}

@article{michelashvili_hierarchical_2020,
	title = {Hierarchical {Timbre}-{Painting} and {Articulation} {Generation}},
	url = {http://arxiv.org/abs/2008.13095},
	abstract = {We present a fast and high-fidelity method for music generation, based on specified f0 and loudness, such that the synthesized audio mimics the timbre and articulation of a target instrument. The generation process consists of learned source-filtering networks, which reconstruct the signal at increasing resolutions. The model optimizes a multi-resolution spectral loss as the reconstruction loss, an adversarial loss to make the audio sound more realistic, and a perceptual f0 loss to align the output to the desired input pitch contour. The proposed architecture enables high-quality fitting of an instrument, given a sample that can be as short as a few minutes, and the method demonstrates state-of-the-art timbre transfer capabilities. Code and audio samples are shared at https://github.com/mosheman5/timbre\_painting.},
	urldate = {2020-11-11},
	journal = {arXiv:2008.13095 [cs, eess]},
	author = {Michelashvili, Michael and Wolf, Lior},
	month = sep,
	year = {2020},
	note = {arXiv: 2008.13095},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	annote = {Comment: accepted in Proc. of the 21st International Society for Music Information Retrieval (ISMIR2020)},
	file = {arXiv Fulltext PDF:/Users/p378593/Zotero/storage/8WLA6SDX/Michelashvili und Wolf - 2020 - Hierarchical Timbre-Painting and Articulation Gene.pdf:application/pdf;arXiv.org Snapshot:/Users/p378593/Zotero/storage/MNLGP9NX/2008.html:text/html}
}

@article{midinet,
	title = {{MidiNet}: {A} {Convolutional} {Generative} {Adversarial} {Network} for {Symbolic}-domain {Music} {Generation}},
	shorttitle = {{MidiNet}},
	url = {http://arxiv.org/abs/1703.10847},
	abstract = {Most existing neural network models for music generation use recurrent neural networks. However, the recent WaveNet model proposed by DeepMind shows that convolutional neural networks (CNNs) can also generate realistic musical waveforms in the audio domain. Following this light, we investigate using CNNs for generating melody (a series of MIDI notes) one bar after another in the symbolic domain. In addition to the generator, we use a discriminator to learn the distributions of melodies, making it a generative adversarial network (GAN). Moreover, we propose a novel conditional mechanism to exploit available prior knowledge, so that the model can generate melodies either from scratch, by following a chord sequence, or by conditioning on the melody of previous bars (e.g. a priming melody), among other possibilities. The resulting model, named MidiNet, can be expanded to generate music with multiple MIDI channels (i.e. tracks). We conduct a user study to compare the melody of eight-bar long generated by MidiNet and by Google's MelodyRNN models, each time using the same priming melody. Result shows that MidiNet performs comparably with MelodyRNN models in being realistic and pleasant to listen to, yet MidiNet's melodies are reported to be much more interesting.},
	urldate = {2020-11-13},
	journal = {arXiv:1703.10847 [cs]},
	author = {Yang, Li-Chia and Chou, Szu-Yu and Yang, Yi-Hsuan},
	month = jul,
	year = {2017},
	note = {arXiv: 1703.10847},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Sound},
	annote = {Comment: 8 pages, Accepted to ISMIR (International Society of Music Information Retrieval) Conference 2017},
	file = {arXiv Fulltext PDF:/Users/p378593/Zotero/storage/VTDUVTUH/Yang et al. - 2017 - MidiNet A Convolutional Generative Adversarial Ne.pdf:application/pdf;arXiv.org Snapshot:/Users/p378593/Zotero/storage/EWD4B84F/1703.html:text/html}
}


@article{chen_gradnorm_2018,
	title = {{GradNorm}: {Gradient} {Normalization} for {Adaptive} {Loss} {Balancing} in {Deep} {Multitask} {Networks}},
	shorttitle = {{GradNorm}},
	url = {http://arxiv.org/abs/1711.02257},
	abstract = {Deep multitask networks, in which one neural network produces multiple predictive outputs, can offer better speed and performance than their single-task counterparts but are challenging to train properly. We present a gradient normalization (GradNorm) algorithm that automatically balances training in deep multitask models by dynamically tuning gradient magnitudes. We show that for various network architectures, for both regression and classification tasks, and on both synthetic and real datasets, GradNorm improves accuracy and reduces overfitting across multiple tasks when compared to single-task networks, static baselines, and other adaptive multitask loss balancing techniques. GradNorm also matches or surpasses the performance of exhaustive grid search methods, despite only involving a single asymmetry hyperparameter \${\textbackslash}alpha\$. Thus, what was once a tedious search process that incurred exponentially more compute for each task added can now be accomplished within a few training runs, irrespective of the number of tasks. Ultimately, we will demonstrate that gradient manipulation affords us great control over the training dynamics of multitask networks and may be one of the keys to unlocking the potential of multitask learning.},
	urldate = {2021-05-27},
	journal = {arXiv:1711.02257 [cs]},
	author = {Chen, Zhao and Badrinarayanan, Vijay and Lee, Chen-Yu and Rabinovich, Andrew},
	month = jun,
	year = {2018},
	note = {arXiv: 1711.02257},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: ICML 2018},
	file = {arXiv Fulltext PDF:/Users/nielswarncke/Zotero/storage/EIUXIB68/Chen et al. - 2018 - GradNorm Gradient Normalization for Adaptive Loss.pdf:application/pdf;arXiv.org Snapshot:/Users/nielswarncke/Zotero/storage/B7VKR8Q7/1711.html:text/html},
}

@article{wang_neural_2019,
	title = {Neural source-filter waveform models for statistical parametric speech synthesis},
	url = {http://arxiv.org/abs/1904.12088},
	abstract = {Neural waveform models such as WaveNet have demonstrated better performance than conventional vocoders for statistical parametric speech synthesis. As an autoregressive (AR) model, WaveNet is limited by a slow sequential waveform generation process. Some new models that use the inverse-autoregressive flow (IAF) can generate a whole waveform in a one-shot manner. However, these IAF-based models require sequential transformation during training, which severely slows down the training speed. Other models such as Parallel WaveNet and ClariNet bring together the benefits of AR and IAF-based models and train an IAF model by transferring the knowledge from a pre-trained AR teacher to an IAF student without any sequential transformation. However, both models require additional training criteria, and their implementation is prohibitively complicated. We propose a framework for neural source-filter (NSF) waveform modeling without AR nor IAF-based approaches. This framework requires only three components for waveform generation: a source module that generates a sine-based signal as excitation, a non-AR dilated-convolution-based filter module that transforms the excitation into a waveform, and a conditional module that pre-processes the acoustic features for the source and filer modules. This framework minimizes spectral-amplitude distances for model training, which can be efficiently implemented by using short-time Fourier transform routines. Under this framework, we designed three NSF models and compared them with WaveNet. It was demonstrated that the NSF models generated waveforms at least 100 times faster than WaveNet, and the quality of the synthetic speech from the best NSF model was better than or equally good as that from WaveNet.},
	urldate = {2021-07-22},
	journal = {arXiv:1904.12088 [cs, eess, stat]},
	author = {Wang, Xin and Takaki, Shinji and Yamagishi, Junichi},
	month = nov,
	year = {2019},
	note = {arXiv: 1904.12088},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning},
	annote = {Comment: Accepted to IEEE/ACM TASLP. Note: this paper is on a follow-up work of our ICASSP paper. Based on the h-NSF introduced in this work, we proposed a h-sinc-NSF model and published the third paper in SSW 10 (https://www.isca-speech.org/archive/SSW\_2019/pdfs/SSW10\_O\_1-1.pdf)},
	file = {arXiv Fulltext PDF:/Users/nielswarncke/Zotero/storage/TTGANEPD/Wang et al. - 2019 - Neural source-filter waveform models for statistic.pdf:application/pdf;arXiv.org Snapshot:/Users/nielswarncke/Zotero/storage/V2ASXIIE/1904.html:text/html},
}

@article{hayes_neural_nodate,
	title = {{NEURAL} {WAVESHAPING} {SYNTHESIS}},
	abstract = {We present the Neural Waveshaping Unit (NEWT): a novel, lightweight, fully causal approach to neural audio synthesis which operates directly in the waveform domain, with an accompanying optimisation (FastNEWT) for efﬁcient CPU inference. The NEWT uses time-distributed multilayer perceptrons with periodic activations to implicitly learn nonlinear transfer functions that encode the characteristics of a target timbre. Once trained, a NEWT can produce complex timbral evolutions by simple afﬁne transformations of its input and output signals. We paired the NEWT with a differentiable noise synthesiser and reverb and found it capable of generating realistic musical instrument performances with only 260k total model parameters, conditioned on F0 and loudness features. We compared our method to state-of-the-art benchmarks with a multistimulus listening test and the Fréchet Audio Distance and found it performed competitively across the tested timbral domains. Our method signiﬁcantly outperformed the benchmarks in terms of generation speed, and achieved real-time performance on a consumer CPU, both with and without FastNEWT, suggesting it is a viable basis for future creative sound design tools.},
	language = {en},
	author = {Hayes, Ben and Saitis, Charalampos and Fazekas, George},
	pages = {8},
	file = {Hayes et al. - NEURAL WAVESHAPING SYNTHESIS.pdf:/Users/nielswarncke/Zotero/storage/574ZC3GP/Hayes et al. - NEURAL WAVESHAPING SYNTHESIS.pdf:application/pdf},
}

@inproceedings{kalchbrenner_efficient_2018,
	title = {Efficient {Neural} {Audio} {Synthesis}},
	url = {http://proceedings.mlr.press/v80/kalchbrenner18a.html},
	language = {en},
	urldate = {2021-07-23},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Kalchbrenner, Nal and Elsen, Erich and Simonyan, Karen and Noury, Seb and Casagrande, Norman and Lockhart, Edward and Stimberg, Florian and Oord, Aaron and Dieleman, Sander and Kavukcuoglu, Koray},
	month = jul,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {2410--2419},
	file = {Full Text PDF:/Users/nielswarncke/Zotero/storage/8HZLXI7F/Kalchbrenner et al. - 2018 - Efficient Neural Audio Synthesis.pdf:application/pdf},
}

@article{kong_diffwave_2021,
	title = {{DiffWave}: {A} {Versatile} {Diffusion} {Model} for {Audio} {Synthesis}},
	shorttitle = {{DiffWave}},
	url = {http://arxiv.org/abs/2009.09761},
	abstract = {In this work, we propose DiffWave, a versatile diffusion probabilistic model for conditional and unconditional waveform generation. The model is non-autoregressive, and converts the white noise signal into structured waveform through a Markov chain with a constant number of steps at synthesis. It is efficiently trained by optimizing a variant of variational bound on the data likelihood. DiffWave produces high-fidelity audios in different waveform generation tasks, including neural vocoding conditioned on mel spectrogram, class-conditional generation, and unconditional generation. We demonstrate that DiffWave matches a strong WaveNet vocoder in terms of speech quality (MOS: 4.44 versus 4.43), while synthesizing orders of magnitude faster. In particular, it significantly outperforms autoregressive and GAN-based waveform models in the challenging unconditional generation task in terms of audio quality and sample diversity from various automatic and human evaluations.},
	urldate = {2021-07-23},
	journal = {arXiv:2009.09761 [cs, eess, stat]},
	author = {Kong, Zhifeng and Ping, Wei and Huang, Jiaji and Zhao, Kexin and Catanzaro, Bryan},
	month = mar,
	year = {2021},
	note = {arXiv: 2009.09761},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning, Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: ICLR 2021 (oral)},
	file = {arXiv Fulltext PDF:/Users/nielswarncke/Zotero/storage/CY8ADAY2/Kong et al. - 2021 - DiffWave A Versatile Diffusion Model for Audio Sy.pdf:application/pdf;arXiv.org Snapshot:/Users/nielswarncke/Zotero/storage/FEJG8G8Y/2009.html:text/html},
}

@inproceedings{prenger_waveglow_2019,
	title = {Waveglow: {A} {Flow}-based {Generative} {Network} for {Speech} {Synthesis}},
	shorttitle = {Waveglow},
	doi = {10.1109/ICASSP.2019.8683143},
	abstract = {In this paper we propose WaveGlow: a flow-based network capable of generating high quality speech from mel-spectrograms. WaveGlow combines insights from Glow [1] and WaveNet [2] in order to provide fast, efficient and high-quality audio synthesis, without the need for auto-regression. WaveGlow is implemented using only a single network, trained using only a single cost function: maximizing the likelihood of the training data, which makes the training procedure simple and stable. Our PyTorch implementation produces audio samples at a rate of more than 500 kHz on an NVIDIA V100 GPU. Mean Opinion Scores show that it delivers audio quality as good as the best publicly available WaveNet implementation. All code will be made publicly available online [3].},
	booktitle = {{ICASSP} 2019 - 2019 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Prenger, Ryan and Valle, Rafael and Catanzaro, Bryan},
	month = may,
	year = {2019},
	note = {ISSN: 2379-190X},
	keywords = {Audio Synthesis, Deep Learning, Generative models, Text-to-speech},
	pages = {3617--3621},
	file = {Submitted Version:/Users/nielswarncke/Zotero/storage/9SPER9LM/Prenger et al. - 2019 - Waveglow A Flow-based Generative Network for Spee.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/nielswarncke/Zotero/storage/BJZ4C9DJ/8683143.html:text/html},
}

@article{donahue_adversarial_2019,
	title = {Adversarial {Audio} {Synthesis}},
	url = {http://arxiv.org/abs/1802.04208},
	abstract = {Audio signals are sampled at high temporal resolutions, and learning to synthesize audio requires capturing structure across a range of timescales. Generative adversarial networks (GANs) have seen wide success at generating images that are both locally and globally coherent, but they have seen little application to audio generation. In this paper we introduce WaveGAN, a first attempt at applying GANs to unsupervised synthesis of raw-waveform audio. WaveGAN is capable of synthesizing one second slices of audio waveforms with global coherence, suitable for sound effect generation. Our experiments demonstrate that, without labels, WaveGAN learns to produce intelligible words when trained on a small-vocabulary speech dataset, and can also synthesize audio from other domains such as drums, bird vocalizations, and piano. We compare WaveGAN to a method which applies GANs designed for image generation on image-like audio feature representations, finding both approaches to be promising.},
	urldate = {2021-07-23},
	journal = {arXiv:1802.04208 [cs]},
	author = {Donahue, Chris and McAuley, Julian and Puckette, Miller},
	month = feb,
	year = {2019},
	note = {arXiv: 1802.04208
version: 3},
	keywords = {Computer Science - Sound, Computer Science - Machine Learning},
	annote = {Comment: Published as a conference paper at ICLR 2019},
	file = {arXiv Fulltext PDF:/Users/nielswarncke/Zotero/storage/YYH9CG8R/Donahue et al. - 2019 - Adversarial Audio Synthesis.pdf:application/pdf;arXiv.org Snapshot:/Users/nielswarncke/Zotero/storage/B3A4UGH4/1802.html:text/html},
}

@article{defossez_music_2021,
	title = {Music {Source} {Separation} in the {Waveform} {Domain}},
	url = {http://arxiv.org/abs/1911.13254},
	abstract = {Source separation for music is the task of isolating contributions, or stems, from different instruments recorded individually and arranged together to form a song. Such components include voice, bass, drums and any other accompaniments.Contrarily to many audio synthesis tasks where the best performances are achieved by models that directly generate the waveform, the state-of-the-art in source separation for music is to compute masks on the magnitude spectrum. In this paper, we compare two waveform domain architectures. We first adapt Conv-Tasnet, initially developed for speech source separation,to the task of music source separation. While Conv-Tasnet beats many existing spectrogram-domain methods, it suffersfrom significant artifacts, as shown by human evaluations. We propose instead Demucs, a novel waveform-to-waveform model,with a U-Net structure and bidirectional LSTM.Experiments on the MusDB dataset show that, with proper data augmentation, Demucs beats allexisting state-of-the-art architectures, including Conv-Tasnet, with 6.3 SDR on average, (and up to 6.8 with 150 extra training songs, even surpassing the IRM oracle for the bass source).Using recent development in model quantization, Demucs can be compressed down to 120MBwithout any loss of accuracy.We also provide human evaluations, showing that Demucs benefit from a large advantagein terms of the naturalness of the audio. However, it suffers from some bleeding,especially between the vocals and other source.},
	urldate = {2021-07-23},
	journal = {arXiv:1911.13254 [cs, eess, stat]},
	author = {Défossez, Alexandre and Usunier, Nicolas and Bottou, Léon and Bach, Francis},
	month = apr,
	year = {2021},
	note = {arXiv: 1911.13254
version: 2},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/nielswarncke/Zotero/storage/W7A6QQVV/Défossez et al. - 2021 - Music Source Separation in the Waveform Domain.pdf:application/pdf;arXiv.org Snapshot:/Users/nielswarncke/Zotero/storage/JR7FYH24/1911.html:text/html},
}

@article{oord_parallel_2017,
	title = {Parallel {WaveNet}: {Fast} {High}-{Fidelity} {Speech} {Synthesis}},
	shorttitle = {Parallel {WaveNet}},
	url = {http://arxiv.org/abs/1711.10433},
	abstract = {The recently-developed WaveNet architecture is the current state of the art in realistic speech synthesis, consistently rated as more natural sounding for many different languages than any previous system. However, because WaveNet relies on sequential generation of one audio sample at a time, it is poorly suited to today's massively parallel computers, and therefore hard to deploy in a real-time production setting. This paper introduces Probability Density Distillation, a new method for training a parallel feed-forward network from a trained WaveNet with no significant difference in quality. The resulting system is capable of generating high-fidelity speech samples at more than 20 times faster than real-time, and is deployed online by Google Assistant, including serving multiple English and Japanese voices.},
	urldate = {2021-07-23},
	journal = {arXiv:1711.10433 [cs]},
	author = {Oord, Aaron van den and Li, Yazhe and Babuschkin, Igor and Simonyan, Karen and Vinyals, Oriol and Kavukcuoglu, Koray and Driessche, George van den and Lockhart, Edward and Cobo, Luis C. and Stimberg, Florian and Casagrande, Norman and Grewe, Dominik and Noury, Seb and Dieleman, Sander and Elsen, Erich and Kalchbrenner, Nal and Zen, Heiga and Graves, Alex and King, Helen and Walters, Tom and Belov, Dan and Hassabis, Demis},
	month = nov,
	year = {2017},
	note = {arXiv: 1711.10433},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/nielswarncke/Zotero/storage/TBEMZD42/Oord et al. - 2017 - Parallel WaveNet Fast High-Fidelity Speech Synthe.pdf:application/pdf;arXiv.org Snapshot:/Users/nielswarncke/Zotero/storage/NZTGHPED/1711.html:text/html},
}

@article{engel_gansynth_2019,
	title = {{GANSynth}: {Adversarial} {Neural} {Audio} {Synthesis}},
	shorttitle = {{GANSynth}},
	url = {http://arxiv.org/abs/1902.08710},
	abstract = {Efficient audio synthesis is an inherently difficult machine learning task, as human perception is sensitive to both global structure and fine-scale waveform coherence. Autoregressive models, such as WaveNet, model local structure at the expense of global latent structure and slow iterative sampling, while Generative Adversarial Networks (GANs), have global latent conditioning and efficient parallel sampling, but struggle to generate locally-coherent audio waveforms. Herein, we demonstrate that GANs can in fact generate high-fidelity and locally-coherent audio by modeling log magnitudes and instantaneous frequencies with sufficient frequency resolution in the spectral domain. Through extensive empirical investigations on the NSynth dataset, we demonstrate that GANs are able to outperform strong WaveNet baselines on automated and human evaluation metrics, and efficiently generate audio several orders of magnitude faster than their autoregressive counterparts.},
	urldate = {2021-07-23},
	journal = {arXiv:1902.08710 [cs, eess, stat]},
	author = {Engel, Jesse and Agrawal, Kumar Krishna and Chen, Shuo and Gulrajani, Ishaan and Donahue, Chris and Roberts, Adam},
	month = apr,
	year = {2019},
	note = {arXiv: 1902.08710},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning, Computer Science - Machine Learning},
	annote = {Comment: Colab Notebook: http://goo.gl/magenta/gansynth-demo},
	file = {arXiv Fulltext PDF:/Users/nielswarncke/Zotero/storage/JP9VV6K2/Engel et al. - 2019 - GANSynth Adversarial Neural Audio Synthesis.pdf:application/pdf;arXiv.org Snapshot:/Users/nielswarncke/Zotero/storage/S9X99PSH/1902.html:text/html},
}

@article{hantrakul_fast_2019,
	title = {{FAST} {AND} {FLEXIBLE} {NEURAL} {AUDIO} {SYNTHESIS}},
	language = {en},
	author = {Hantrakul, Lamtharn and Engel, Jesse and Roberts, Adam and Gu, Chenjie},
	year = {2019},
	pages = {7},
	file = {Hantrakul et al. - 2019 - FAST AND FLEXIBLE NEURAL AUDIO SYNTHESIS.pdf:/Users/nielswarncke/Zotero/storage/MTPX9R7Y/Hantrakul et al. - 2019 - FAST AND FLEXIBLE NEURAL AUDIO SYNTHESIS.pdf:application/pdf},
}

@article{hantrakul_fast_2019-1,
	title = {{FAST} {AND} {FLEXIBLE} {NEURAL} {AUDIO} {SYNTHESIS}},
	language = {en},
	author = {Hantrakul, Lamtharn and Engel, Jesse and Roberts, Adam and Gu, Chenjie},
	year = {2019},
	pages = {7},
	file = {Hantrakul et al. - 2019 - FAST AND FLEXIBLE NEURAL AUDIO SYNTHESIS.pdf:/Users/nielswarncke/Zotero/storage/GTB986ZC/Hantrakul et al. - 2019 - FAST AND FLEXIBLE NEURAL AUDIO SYNTHESIS.pdf:application/pdf},
}

@article{chung_empirical_2014,
	title = {Empirical {Evaluation} of {Gated} {Recurrent} {Neural} {Networks} on {Sequence} {Modeling}},
	url = {http://arxiv.org/abs/1412.3555},
	abstract = {In this paper we compare different types of recurrent units in recurrent neural networks (RNNs). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found GRU to be comparable to LSTM.},
	urldate = {2021-07-23},
	journal = {arXiv:1412.3555 [cs]},
	author = {Chung, Junyoung and Gulcehre, Caglar and Cho, KyungHyun and Bengio, Yoshua},
	month = dec,
	year = {2014},
	note = {arXiv: 1412.3555},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: Presented in NIPS 2014 Deep Learning and Representation Learning Workshop},
	file = {arXiv Fulltext PDF:/Users/nielswarncke/Zotero/storage/ELWJ6CCG/Chung et al. - 2014 - Empirical Evaluation of Gated Recurrent Neural Net.pdf:application/pdf;arXiv.org Snapshot:/Users/nielswarncke/Zotero/storage/TFCIXCJQ/1412.html:text/html},
}

@article{wang_deep_2019,
	title = {Deep {Text}-to-{Speech} {System} with {Seq2Seq} {Model}},
	url = {http://arxiv.org/abs/1903.07398},
	abstract = {Recent trends in neural network based text-to-speech/speech synthesis pipelines have employed recurrent Seq2seq architectures that can synthesize realistic sounding speech directly from text characters. These systems however have complex architectures and takes a substantial amount of time to train. We introduce several modifications to these Seq2seq architectures that allow for faster training time, and also allows us to reduce the complexity of the model architecture at the same time. We show that our proposed model can achieve attention alignment much faster than previous architectures and that good audio quality can be achieved with a model that's much smaller in size. Sample audio available at https://soundcloud.com/gary-wang-23/sets/tts-samples-for-cmpt-419.},
	urldate = {2021-08-13},
	journal = {arXiv:1903.07398 [cs]},
	author = {Wang, Gary},
	month = mar,
	year = {2019},
	note = {arXiv: 1903.07398},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/nielswarncke/Zotero/storage/UARZ5HM9/Wang - 2019 - Deep Text-to-Speech System with Seq2Seq Model.pdf:application/pdf;arXiv.org Snapshot:/Users/nielswarncke/Zotero/storage/CTWT45ZU/1903.html:text/html},
}

@article{dhariwal_diffusion_2021,
	title = {Diffusion {Models} {Beat} {GANs} on {Image} {Synthesis}},
	url = {http://arxiv.org/abs/2105.05233},
	abstract = {We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128\${\textbackslash}times\$128, 4.59 on ImageNet 256\${\textbackslash}times\$256, and 7.72 on ImageNet 512\${\textbackslash}times\$512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256\${\textbackslash}times\$256 and 3.85 on ImageNet 512\${\textbackslash}times\$512. We release our code at https://github.com/openai/guided-diffusion},
	urldate = {2021-08-13},
	journal = {arXiv:2105.05233 [cs, stat]},
	author = {Dhariwal, Prafulla and Nichol, Alex},
	month = jun,
	year = {2021},
	note = {arXiv: 2105.05233},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	annote = {Comment: Added compute requirements, ImageNet 256\${\textbackslash}times\$256 upsampling FID and samples, DDIM guided sampler, fixed typos},
	file = {arXiv Fulltext PDF:/Users/nielswarncke/Zotero/storage/XZK7FW54/Dhariwal and Nichol - 2021 - Diffusion Models Beat GANs on Image Synthesis.pdf:application/pdf;arXiv.org Snapshot:/Users/nielswarncke/Zotero/storage/7Y5DA94D/2105.html:text/html},
}

@inproceedings{liu_neural_2020,
	title = {Neural {Homomorphic} {Vocoder}},
	url = {http://www.isca-speech.org/archive/Interspeech_2020/abstracts/3188.html},
	doi = {10.21437/Interspeech.2020-3188},
	abstract = {In this paper, we propose the neural homomorphic vocoder (NHV), a source-ﬁlter model based neural vocoder framework. NHV synthesizes speech by ﬁltering impulse trains and noise with linear time-varying (LTV) ﬁlters. A neural network controls the LTV ﬁlters by estimating complex cepstrums of time-varying impulse responses given acoustic features. The proposed framework can be trained with a combination of multi-resolution STFT loss and adversarial loss functions. Due to the use of DSP-based synthesis methods, NHV is highly efﬁcient, fully controllable and interpretable. A vocoder was built under the framework to synthesize speech given log-Mel spectrograms and fundamental frequencies. While the model cost only 15 kFLOPs per sample, the synthesis quality remained comparable to baseline neural vocoders in both copy-synthesis and text-to-speech.},
	language = {en},
	urldate = {2021-08-14},
	booktitle = {Interspeech 2020},
	publisher = {ISCA},
	author = {Liu, Zhijun and Chen, Kuan and Yu, Kai},
	month = oct,
	year = {2020},
	pages = {240--244},
	file = {Liu et al. - 2020 - Neural Homomorphic Vocoder.pdf:/Users/nielswarncke/Zotero/storage/92EEMSU3/Liu et al. - 2020 - Neural Homomorphic Vocoder.pdf:application/pdf},
}

@article{mccarthy_hooligan_2020,
	title = {{HooliGAN}: {Robust}, {High} {Quality} {Neural} {Vocoding}},
	shorttitle = {{HooliGAN}},
	url = {http://arxiv.org/abs/2008.02493},
	abstract = {Recent developments in generative models have shown that deep learning combined with traditional digital signal processing (DSP) techniques could successfully generate convincing violin samples [1], that source-excitation combined with WaveNet yields high-quality vocoders [2, 3] and that generative adversarial network (GAN) training can improve naturalness [4, 5]. By combining the ideas in these models we introduce HooliGAN, a robust vocoder that has state of the art results, finetunes very well to smaller datasets ({\textless}30 minutes of speechdata) and generates audio at 2.2MHz on GPU and 35kHz on CPU. We also show a simple modification to Tacotron-basedmodels that allows seamless integration with HooliGAN. Results from our listening tests show the proposed model's ability to consistently output high-quality audio with a variety of datasets, big and small. We provide samples at the following demo page: https://resemble-ai.github.io/hooligan\_demo/},
	urldate = {2021-08-14},
	journal = {arXiv:2008.02493 [cs, eess]},
	author = {McCarthy, Ollie and Ahmed, Zohaib},
	month = aug,
	year = {2020},
	note = {arXiv: 2008.02493},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {arXiv Fulltext PDF:/Users/nielswarncke/Zotero/storage/G23MEBGD/McCarthy and Ahmed - 2020 - HooliGAN Robust, High Quality Neural Vocoding.pdf:application/pdf;arXiv.org Snapshot:/Users/nielswarncke/Zotero/storage/D2YUCXRP/2008.html:text/html},
}

@article{li_creating_2019,
	title = {Creating a {Multitrack} {Classical} {Music} {Performance} {Dataset} for {Multimodal} {Music} {Analysis}: {Challenges}, {Insights}, and {Applications}},
	volume = {21},
	issn = {1520-9210, 1941-0077},
	shorttitle = {Creating a {Multitrack} {Classical} {Music} {Performance} {Dataset} for {Multimodal} {Music} {Analysis}},
	url = {https://ieeexplore.ieee.org/document/8411155/},
	doi = {10.1109/TMM.2018.2856090},
	abstract = {We introduce a dataset for facilitating audio-visual analysis of music performances. The dataset comprises 44 simple multi-instrument classical music pieces assembled from coordinated but separately recorded performances of individual tracks. For each piece, we provide the musical score in MIDI format, the audio recordings of the individual tracks, the audio and video recording of the assembled mixture, and ground-truth annotation ﬁles including frame-level and note-level transcriptions. We describe our methodology for the creation of the dataset, particularly highlighting our approaches for addressing the challenges involved in maintaining synchronization and expressiveness. We demonstrate the high quality of synchronization achieved with our proposed approach by comparing the dataset with existing widely-used music audio datasets.},
	language = {en},
	number = {2},
	urldate = {2021-08-16},
	journal = {IEEE Transactions on Multimedia},
	author = {Li, Bochen and Liu, Xinzhao and Dinesh, Karthik and Duan, Zhiyao and Sharma, Gaurav},
	month = feb,
	year = {2019},
	pages = {522--535},
	file = {Li et al. - 2019 - Creating a Multitrack Classical Music Performance .pdf:/Users/nielswarncke/Zotero/storage/YURZZNEW/Li et al. - 2019 - Creating a Multitrack Classical Music Performance .pdf:application/pdf},
}

@article{turian_im_nodate,
	title = {I’m {Sorry} for {Your} {Loss}: {Spectrally}-{Based} {Audio} {Distances} {Are} {Bad} at {Pitch}},
	abstract = {Growing research demonstrates that synthetic failure modes imply poor generalization. We compare commonly used audio-to-audio losses on a synthetic benchmark, measuring the pitch distance between two stationary sinusoids. The results are surprising: many have poor sense of pitch direction. These shortcomings are exposed using simple rank assumptions. Our task is trivial for humans but difﬁcult for these audio distances, suggesting signiﬁcant progress can be made in self-supervised audio learning by improving current losses.},
	language = {en},
	author = {Turian, Joseph and Henry, Max},
	pages = {16},
	file = {Turian and Henry - I’m Sorry for Your Loss Spectrally-Based Audio Di.pdf:/Users/nielswarncke/Zotero/storage/56C4MCJV/Turian and Henry - I’m Sorry for Your Loss Spectrally-Based Audio Di.pdf:application/pdf},
}

@article{roberts_hierarchical_2019,
	title = {A {Hierarchical} {Latent} {Vector} {Model} for {Learning} {Long}-{Term} {Structure} in {Music}},
	url = {http://arxiv.org/abs/1803.05428},
	abstract = {The Variational Autoencoder (VAE) has proven to be an effective model for producing semantically meaningful latent representations for natural data. However, it has thus far seen limited application to sequential data, and, as we demonstrate, existing recurrent VAE models have difficulty modeling sequences with long-term structure. To address this issue, we propose the use of a hierarchical decoder, which first outputs embeddings for subsequences of the input and then uses these embeddings to generate each subsequence independently. This structure encourages the model to utilize its latent code, thereby avoiding the "posterior collapse" problem, which remains an issue for recurrent VAEs. We apply this architecture to modeling sequences of musical notes and find that it exhibits dramatically better sampling, interpolation, and reconstruction performance than a "flat" baseline model. An implementation of our "MusicVAE" is available online at http://g.co/magenta/musicvae-code.},
	urldate = {2021-09-03},
	journal = {arXiv:1803.05428 [cs, eess, stat]},
	author = {Roberts, Adam and Engel, Jesse and Raffel, Colin and Hawthorne, Curtis and Eck, Douglas},
	month = nov,
	year = {2019},
	note = {arXiv: 1803.05428},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning},
	annote = {Comment: ICML Camera Ready Version (w/ fixed typos)},
	file = {arXiv Fulltext PDF:/Users/nielswarncke/Zotero/storage/WFGYCJD8/Roberts et al. - 2019 - A Hierarchical Latent Vector Model for Learning Lo.pdf:application/pdf;arXiv.org Snapshot:/Users/nielswarncke/Zotero/storage/4ZIUISRB/1803.html:text/html},
}

@article{kim_crepe_2018,
	title = {{CREPE}: {A} {Convolutional} {Representation} for {Pitch} {Estimation}},
	shorttitle = {{CREPE}},
	url = {http://arxiv.org/abs/1802.06182},
	abstract = {The task of estimating the fundamental frequency of a monophonic sound recording, also known as pitch tracking, is fundamental to audio processing with multiple applications in speech processing and music information retrieval. To date, the best performing techniques, such as the pYIN algorithm, are based on a combination of DSP pipelines and heuristics. While such techniques perform very well on average, there remain many cases in which they fail to correctly estimate the pitch. In this paper, we propose a data-driven pitch tracking algorithm, CREPE, which is based on a deep convolutional neural network that operates directly on the time-domain waveform. We show that the proposed model produces state-of-the-art results, performing equally or better than pYIN. Furthermore, we evaluate the model's generalizability in terms of noise robustness. A pre-trained version of CREPE is made freely available as an open-source Python module for easy application.},
	urldate = {2021-09-03},
	journal = {arXiv:1802.06182 [cs, eess, stat]},
	author = {Kim, Jong Wook and Salamon, Justin and Li, Peter and Bello, Juan Pablo},
	month = feb,
	year = {2018},
	note = {arXiv: 1802.06182},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning},
	annote = {Comment: ICASSP 2018},
	file = {arXiv Fulltext PDF:/Users/nielswarncke/Zotero/storage/H5J6A2GP/Kim et al. - 2018 - CREPE A Convolutional Representation for Pitch Es.pdf:application/pdf;arXiv.org Snapshot:/Users/nielswarncke/Zotero/storage/YL9KQ5NM/1802.html:text/html},
}
@article{lstm,
    author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
    year = {1997},
    month = {12},
    pages = {1735-80},
    title = {Long Short-term Memory},
    volume = {9},
    journal = {Neural computation},
    doi = {10.1162/neco.1997.9.8.1735}
}


@article{original_transfer,
    author = {Bozinovski, S and Fulgosi, A.},
    year = {1976},
    title = {The influence of pattern similarity and transfer of learning upon training of a base perceptron B2. (original in Croatian: Utjecaj slicnosti likova i transfera ucenja na obucavanje baznog perceptrona B2)},
    journal = {Proc. Symp. Informatica},
}



@article{idmt_bass,
    author = {Abeßer, Jakob and Dittmar, Christian and Kramer, Patrick and Schuller, Gerald},
    year = {2013},
    title = {IDMT-SMT-Bass-Single-Track},
	url = {https://www.idmt.fraunhofer.de/en/business_units/m2d/smt/bass_lines.html},
    journal = {16th International Conference on Digital Audio Effects (DAFx)}
}


@article{idmt_guitar,
    author = {Kehling, Christian and Abeßer, Jakob and Dittmar, Christian and Schuller, Gerald},
    year = {2014},
    title = {Automatic Tablature Transcription of Electric Guitar Recordings by Estimation of Score- and Instrument-related Parameters},
	url = {https://www.idmt.fraunhofer.de/en/business_units/m2d/smt/guitar.html},
    journal = {Proceedings of the 17th International Conference on Digital Audio Effects (DAFx, 2014)}
}



@article{idmt_drum,
    author = {Dittmar, Christian and Gärtner, Daniel},
    year = {2014},
    title = {IDMT-SMT-Drums},
	url = {https://www.idmt.fraunhofer.de/en/business_units/m2d/smt/drums.html},
    journal = {Proceedings of the 17th International Conference on Digital Audio Effects (DAFx, 2014)}
}

@misc{performance-rnn-2017,
    author = {Ian Simon and Sageev Oore},
    title = {
        Performance RNN: Generating Music with Expressive Timing and Dynamics
    },
    journal = {Magenta Blog},
    type = {Blog},
    year = {2017},
    howpublished = {\url{https://magenta.tensorflow.org/performance-rnn}}
}
